{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6222ef-bcf5-4607-b6e3-a8adc1526d98",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97832a81-e9e9-4399-a89e-834d45c65b8f",
   "metadata": {},
   "source": [
    "#### What is RAG?\n",
    "**Retrieval Augmented Generation** is a way to improve how AI models, like chatbots, generate the text. It combines the AI's ability to create text with a system that finds and uses relevant information from a database/knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4b539-e522-4070-9c31-391aa7094aa6",
   "metadata": {},
   "source": [
    "#### How Does RAG Work?\n",
    "1. **You ask a question/query**: You ask the Chatbot something.\n",
    "2. **Find relevant information for you**: The Chatbot searches a knowledge-base to find the most relevant information related to your question/query.\n",
    "3. **Genearate accurate response**: The Chatbot uses this information to create a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "68080cd7-47ae-4bea-95ba-4a545a38bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (25.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gptqmodel 1.9.0 requires numpy>=2.2.2, but you have numpy 2.1.3 which is incompatible.\n",
      "gptqmodel 1.9.0 requires protobuf>=5.29.3, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
      "gptqmodel 1.9.0 requires protobuf>=5.29.3, but you have protobuf 4.25.7 which is incompatible.\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install tf-keras --upgrade -q\n",
    "!pip install --upgrade transformers numpy sentence-transformers langchain_community langchain langchain_community chromadb cmake -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "72f92b7f-76df-4e70-a143-30469752b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries and modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, List\n",
    "from pydantic import Field, BaseModel, Extra\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb import PersistentClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import FakeEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, LLMChain, ConversationalRetrievalChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationBufferMemory\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e9ee5641-b5de-4123-b388-907beabb3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initializing the vectorizer\n",
    "# \"all-MiniLM-L6-v2\": is a good general-purpose embedding model that balances performance and efficiency\n",
    "vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # This vectorizer converts text into vectors in embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a200a0-fdcd-4844-9d35-4e0c4ff22828",
   "metadata": {},
   "source": [
    "# RAG with multiple Policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d853221d-029a-417f-9343-67b7970f7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV Report with Policy Metadata\n",
    "\n",
    "def load_policy_report(csv_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the report.csv that includes policy titles and reference URLs.\n",
    "    Expected CSV columns: 'Title', 'URL', and potentially others.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try reading with the default engine and UTF-8 encoding\n",
    "        df = pd.read_csv(csv_file, encoding=\"utf-8\", sep=\"\\t\", on_bad_lines='skip')\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Failed to decode with encoding utf-8. Trying 'utf-16' instead.\")\n",
    "        df = pd.read_csv(csv_file, encoding=\"utf-16\", sep=\"\\t\", on_bad_lines='skip')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "34616ca1-5e29-456e-839e-d0e8dfb19b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode with encoding utf-8. Trying 'utf-16' instead.\n",
      "Report loaded: 429 policies found in report.csv.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file (adjust the path if needed)\n",
    "report_df = load_policy_report(\"report.csv\")\n",
    "print(f\"Report loaded: {len(report_df)} policies found in report.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f506444a-c810-4ed2-b097-31aee5373637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode with encoding utf-8. Trying 'utf-16' instead.\n",
      "[INFO]: Report loaded with 428 policies.\n"
     ]
    }
   ],
   "source": [
    "def map_policy_metadata(report_df: pd.DataFrame) -> dict:\n",
    "    \"\"\" Helper function that creates a mapping where keys are lowercase policy titles (from the CSV) and values are the corresponding URL. \"\"\"\n",
    "    mapping = {}\n",
    "    for _, row in report_df.iterrows():\n",
    "        title = row[\"Title\"].strip().lower()\n",
    "        url = row[\"URL\"].strip()\n",
    "        mapping[title] = url\n",
    "    return mapping\n",
    "\n",
    "# Load CSV and create mapping.\n",
    "report_df = load_policy_report(\"report.csv\")\n",
    "policy_mapping = map_policy_metadata(report_df)\n",
    "print(f\"[INFO]: Report loaded with {len(policy_mapping)} policies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6998c88c-eec7-4378-a9d9-63b71d95c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3139 document pages from policies.\n"
     ]
    }
   ],
   "source": [
    "# First, I'm trying to all the Policy documents (pdfs), chunking them while preserving the metadata of each policy document.\n",
    "def load_policies(folder_path: str, policy_mapping: dict) -> List[Document]:\n",
    "    \"\"\" Helper function that loads all PDFs from the folder and update each Document's metadata with. \"\"\"\n",
    "    all_docs = []\n",
    "    for pdf_path in Path(folder_path).glob(\"*.pdf\"):\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        docs = loader.load()  \n",
    "        file_title = pdf_path.stem.lower()\n",
    "        matched_url = None\n",
    "        matched_policy_title = None\n",
    "\n",
    "        for title_key, url in policy_mapping.items():\n",
    "            if title_key in file_title:\n",
    "                matched_url = url\n",
    "                matched_policy_title = title_key  \n",
    "                break\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source_file\"] = pdf_path.name\n",
    "            if matched_url:\n",
    "                doc.metadata[\"policy_title\"] = matched_policy_title\n",
    "                doc.metadata[\"policy_url\"] = matched_url\n",
    "            else:\n",
    "                # Fallback if no match is found in the CSV.\n",
    "                doc.metadata[\"policy_title\"] = pdf_path.stem\n",
    "                doc.metadata[\"policy_url\"] = None\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "raw_documents = load_policies(\"Policies/\", policy_mapping)\n",
    "print(f\"Loaded {len(raw_documents)} document pages from policies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "46c7745b-61be-4213-a44c-4beb440e4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Loaded 3139 documents (pages) from the 'Policies/' folder.\n"
     ]
    }
   ],
   "source": [
    "print(f\"[INFO]: Loaded {len(raw_documents)} documents (pages) from the 'Policies/' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2039c930-ec6d-4009-96c6-946fc4f66351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking each document (policy) page while preserving the metadata\n",
    "def chunk_documents(raw_docs: list[Document], chunk_size: int = 500, overlap: int = 50) -> list[Document]:\n",
    "    \"\"\" Helper function that chunks each document and preserve the metadata. \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "    for doc in raw_docs:\n",
    "        doc_chunks = splitter.split_documents([doc])\n",
    "        for chunk in doc_chunks:\n",
    "            chunk.metadata = doc.metadata.copy()  # Preserve original metadata\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5ea3a9d1-a643-4f50-b596-b19ff57fae02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Chunked into 18115 segments.\n"
     ]
    }
   ],
   "source": [
    "chunked_docs = chunk_documents(raw_documents)\n",
    "print(f\"[INFO]: Chunked into {len(chunked_docs)} segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e15ef-a511-4745-9121-c6507b4d1dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9ca7ffcd-4bdf-431a-83f4-5cdd5fd58877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_filter_metadata(metadata: dict, allowed_types=(str, int, float, bool)) -> dict:\n",
    "    \"\"\" \n",
    "    Helper function that filters a metadata dictionary so that each value is of type str, int, float, or bool.\n",
    "    If a value is not one of these types (and not None), it's converted to a string.\n",
    "    Keys with None values are dropped.\n",
    "    \"\"\"\n",
    "    filtered = {}\n",
    "    for key, value in metadata.items():\n",
    "        if value is None:\n",
    "            continue  # Skip None values\n",
    "        if isinstance(value, allowed_types):\n",
    "            filtered[key] = value\n",
    "        else:\n",
    "            # Optionally, convert the value to a string.\n",
    "            filtered[key] = str(value)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e5cb19ee-a484-42a5-a286-621426355163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Semantic retriever set up.\n"
     ]
    }
   ],
   "source": [
    "# Building Semantic & Keyword based Retriever\n",
    "\n",
    "# After chunking, I'm filtering metadata for each document chunk.\n",
    "# I'm replacing any None values with a default, or removes keys with non-simple types.\n",
    "for doc in chunked_docs:\n",
    "    doc.metadata = simple_filter_metadata(doc.metadata)\n",
    "\n",
    "db_semantic = Chroma.from_documents(\n",
    "    chunked_docs, \n",
    "    vectorizer,\n",
    "    client=PersistentClient(path=\"./chroma_db\")\n",
    ")\n",
    "semantic_retriever = db_semantic.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"[INFO]: Semantic retriever set up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ec9b438-8068-4f7b-b92b-9393f0a9082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Keyword retriever set up.\n"
     ]
    }
   ],
   "source": [
    "# Using FakeEmbeddings for keyword search (BM25-like retrieval)\n",
    "\n",
    "db_keyword = FAISS.from_documents(chunked_docs, FakeEmbeddings(size=768))\n",
    "keyword_retriever = db_keyword.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"[INFO]: Keyword retriever set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bad25-e9b6-46af-b50c-c031e214b330",
   "metadata": {},
   "source": [
    "#### Building GraphRAG Component\n",
    "\n",
    "Overhere, I'm trying to build a graph over document chunk using NetworkX, where nodes represent chunks and edges connect similar chunks.  This graph helps propagate context across policy boundaries. \n",
    "\n",
    "This graph leverages approximate nearest neighbor search via FAISS to build a sparse graph over our policy documents chunks. This approach avoids computing all pairwise similarities (which can be prohibitively expensive for 500+ PDFs) by efficiently retrieving only the nearest neighbors for each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a4fa1967-cc06-40cf-b910-4d08aaae4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Approach: I'm using FAISS IndexIVFFlat to perform approximate nearest neighbor search. Over here, an edge is added between two nodes if their inner product (cosine similarity) exceeds the specified threshold.\n",
    "'''\n",
    "\n",
    "def build_policy_graph(\n",
    "    docs: list[Document], \n",
    "    vectorizer, \n",
    "    k_neighbors: int = 5, \n",
    "    threshold: float = 0.9, \n",
    "    nlist: int = 100\n",
    ") -> nx.Graph:\n",
    "    \"\"\" Helper function that build a graph. \"\"\"\n",
    "\n",
    "    # Computing embeddings for each document chunk\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        emb = np.array(vectorizer.embed_query(doc.page_content)).astype(\"float32\")\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.stack(embeddings)\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Building an approximate FAISS index with inner-product\n",
    "    quantizer = faiss.IndexFlatIP(dim)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "    index.train(embeddings)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Retrieving approximate k_neighbors for each chunk (include self in results)\n",
    "    # returns: D = distances, I = Indices\n",
    "    D, I = index.search(embeddings, k_neighbors + 1)\n",
    "\n",
    "    # Building a graph based on neighbors with similarity above threshold\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Adding nodes with metadata (source and page)\n",
    "    for idx, doc in enumerate(docs):\n",
    "        G.add_node(idx, doc=doc)\n",
    "\n",
    "    # For each document, adding edges from its approximate neighbors (skipping self)\n",
    "    for i, (neighbors, distances) in enumerate(zip(I, D)):\n",
    "        for j, sim in zip(neighbors[1:], distances[1:]):\n",
    "            if sim >= threshold:\n",
    "                # Adding an edge with weight=similarity\n",
    "                G.add_edge(i, j, weight=float(sim))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "571caf99-1373-457b-8bc0-6ba13d33efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Approximate graph created with 18115 nodes and 7435 edges.\n"
     ]
    }
   ],
   "source": [
    "# Building the graph on our chunked documents \n",
    "\n",
    "policy_graph = build_policy_graph(chunked_docs, vectorizer, k_neighbors=5, threshold=0.9, nlist=100)\n",
    "print(f\"[INFO]: Approximate graph created with {policy_graph.number_of_nodes()} nodes and {policy_graph.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c2189987-a57d-45e8-8119-f5257e3b32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Summary:\n",
      "  Number of nodes: 18115\n",
      "  Number of edges: 7435\n",
      "\n",
      "Sample Nodes (first 5):\n",
      "(0, {'doc': Document(metadata={'producer': 'Prince 12.5.1 (www.princexml.com)', 'creator': 'PolicyStat', 'creationdate': '', 'subject': 'The California State University', 'author': 'Grommo, April: Asst VC, Enroll Mgmt Srvcs', 'title': '2021 – 2022 Emergency Grant Allocation', 'source': 'Policies/2021 - 2022 Emergency Grant Allocation.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': '2021 - 2022 Emergency Grant Allocation.pdf', 'policy_title': '2021 - 2022 Emergency Grant Allocation'}, page_content='COPY\\nStatus Active PolicyStat ID 10719972 \\nOrigination 12/7/2021 \\nEffective 12/7/2021 \\nReviewed 12/7/2021 \\nNext Review 12/7/2023 \\nOwner April Grommo: \\nAsst VC, Enroll \\nMgmt Srvcs \\nArea Academic and \\nStudent Affairs \\n2021 – 2022 Emergency Grant Allocation \\nPolicy \\nThis policy provides procedural guidance related to the allocation of $30 million of one-time funding \\nissued under the Budget Act of 2021 for emergency financial assistance grants for low-income students.')})\n",
      "(1, {'doc': Document(metadata={'producer': 'Prince 12.5.1 (www.princexml.com)', 'creator': 'PolicyStat', 'creationdate': '', 'subject': 'The California State University', 'author': 'Grommo, April: Asst VC, Enroll Mgmt Srvcs', 'title': '2021 – 2022 Emergency Grant Allocation', 'source': 'Policies/2021 - 2022 Emergency Grant Allocation.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': '2021 - 2022 Emergency Grant Allocation.pdf', 'policy_title': '2021 - 2022 Emergency Grant Allocation'}, page_content='The $30 million is allocated to campuses by a pro-rata distribution based on the number of 2019-20 \\nstudents who were eligible to receive Pell Grant financial aid, as well as those who met the requirements \\nfor an exemption from paying nonresident tuition and the income criteria applicable to the California \\nDream Act application. \\nStudent Grant Eligibility \\nGrants are available to students that meet the following conditions:')})\n",
      "(2, {'doc': Document(metadata={'producer': 'Prince 12.5.1 (www.princexml.com)', 'creator': 'PolicyStat', 'creationdate': '', 'subject': 'The California State University', 'author': 'Grommo, April: Asst VC, Enroll Mgmt Srvcs', 'title': '2021 – 2022 Emergency Grant Allocation', 'source': 'Policies/2021 - 2022 Emergency Grant Allocation.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': '2021 - 2022 Emergency Grant Allocation.pdf', 'policy_title': '2021 - 2022 Emergency Grant Allocation'}, page_content='1. The student is currently enrolled in at least 6 semester units, or the quarter equivalent. \\n2. The student is able to demonstrate an emergency financial need, including loss of \\nemployment. \\n3. The student either currently qualifies as low income by meeting requirements to receive Pell \\nGrant financial aid for the upcoming semester or by meeting all the requirements for an \\nexemption from paying nonresident tuition pursuant to Section 68130.5 of the Education Code')})\n",
      "(3, {'doc': Document(metadata={'producer': 'Prince 12.5.1 (www.princexml.com)', 'creator': 'PolicyStat', 'creationdate': '', 'subject': 'The California State University', 'author': 'Grommo, April: Asst VC, Enroll Mgmt Srvcs', 'title': '2021 – 2022 Emergency Grant Allocation', 'source': 'Policies/2021 - 2022 Emergency Grant Allocation.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': '2021 - 2022 Emergency Grant Allocation.pdf', 'policy_title': '2021 - 2022 Emergency Grant Allocation'}, page_content=\"and the income criteria applicable to the California Dream Act application. \\n4. The student has either: \\na. Earned a grade point average of at least 2.0 in one of the student's previous three \\nsemester terms or in one of their previous four quarters, irrespective of whether that \\nterm occurred at the student's prior, or current, local educational agency, community \\ncollege, or four-year college; or \\nb. The student is disabled and receiving additional support or services through a\")})\n",
      "(4, {'doc': Document(metadata={'producer': 'Prince 12.5.1 (www.princexml.com)', 'creator': 'PolicyStat', 'creationdate': '', 'subject': 'The California State University', 'author': 'Grommo, April: Asst VC, Enroll Mgmt Srvcs', 'title': '2021 – 2022 Emergency Grant Allocation', 'source': 'Policies/2021 - 2022 Emergency Grant Allocation.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': '2021 - 2022 Emergency Grant Allocation.pdf', 'policy_title': '2021 - 2022 Emergency Grant Allocation'}, page_content='campus program for disabled students. \\nIn providing an emergency financial assistance grant to a student, to the extent that data is readily \\n2021 – 2022 Emergency Grant Allocation. Retrieved 3/27/2025. Official copy at http://calstate.policystat.com/policy/\\n10719972/. Copyright © 2025 The California State University\\nPage 1 of 3')})\n",
      "\n",
      "Sample Edges (first 5):\n",
      "(10, np.int64(2915), {'weight': 0.9395480155944824})\n",
      "(13, np.int64(8004), {'weight': 0.9599432349205017})\n",
      "(13, np.int64(8012), {'weight': 0.9582447409629822})\n",
      "(13, np.int64(10733), {'weight': 0.9572423696517944})\n",
      "(13, np.int64(11654), {'weight': 0.9570760726928711})\n"
     ]
    }
   ],
   "source": [
    "def print_policy_graph_info(graph):\n",
    "    ''' Helper function that return the summary of the Policy Graph. '''\n",
    "    print(\"Graph Summary:\")\n",
    "    print(f\"  Number of nodes: {graph.number_of_nodes()}\")\n",
    "    print(f\"  Number of edges: {graph.number_of_edges()}\")\n",
    "    print(\"\\nSample Nodes (first 5):\")\n",
    "    for node in list(graph.nodes(data=True))[:5]:\n",
    "        print(node)\n",
    "    print(\"\\nSample Edges (first 5):\")\n",
    "    for edge in list(graph.edges(data=True))[:5]:\n",
    "        print(edge)\n",
    "\n",
    "print_policy_graph_info(policy_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6fc60e82-a51f-4aae-bafe-882e781638c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_prompt_template = \"\"\"\n",
    "# You are a highly knowledgeable assistant with expertise in CSU policies. Your task is to answer the following question using the context provided.\n",
    "# IMPORTANT: If the question is not related to CSU policies, respond with: \n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "# Question: {question}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# question_prompt_template = \"\"\"\n",
    "# You are a highly knowledgeable assistant with deep expertise in CSU policies. Your responses must strictly pertain to CSU policies and internal policy matters.\n",
    "# IMPORTANT: If the user's question is not about CSU policies or policy-related information, immediately respond with:\n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "# Do not provide any additional content in that case.\n",
    "# Otherwise, answer the following question using the context provided.\n",
    "# Question: {query}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# question_prompt = PromptTemplate(\n",
    "#     input_variables=[\"question\", \"context\"],\n",
    "#     template=\"\"\"\n",
    "# You are a highly knowledgeable assistant with deep expertise in CSU policies.\n",
    "# Only answer questions related to CSU policies.\n",
    "# If the question is not related to CSU policies, respond with:\n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "\n",
    "# Question: {question}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\",\n",
    "# )\n",
    "\n",
    "# question_prompt_template = \"\"\"\n",
    "# You are a CSU Policy Assistant. You are only allowed to answer questions directly related to California State University policies using official policy documents as your source.\n",
    "# If the user's question is not related to CSU policies, respond exactly with:\n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "# If the user's question is unclear, ask for clarification.\n",
    "# Otherwise, answer the question using the context provided.\n",
    "\n",
    "# Question: {question}\n",
    "# Context: {context}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "question_prompt_template = \"\"\"\n",
    "You are a CSU Policy Assistant. Your job is to answer ONLY questions that are directly about California State University (CSU) policies, using official policy documents as your source.\n",
    "\n",
    "Step 1: Before answering, check if the user's question is about CSU policies.\n",
    "- If the question is NOT about CSU policies, respond ONLY with:\n",
    "\"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "- Do NOT attempt to answer or provide any information outside CSU policies.\n",
    "\n",
    "Step 2: If the question IS about CSU policies:\n",
    "- If unclear, ask the user to clarify.\n",
    "- Otherwise, answer using the provided context.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "355651ba-a157-4fb2-93f9-0d244d6c5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine_prompt_template = \"\"\"\n",
    "# The initial answer is: {existing_answer}\n",
    "# Additional context: {context}\n",
    "# IMPORTANT: Ensure the question is clearly related to CSU policies. \n",
    "# If it is not, respond with: \n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please ask a question related to CSU policies?\"\n",
    "# Otherwise, refine and elaborate on the answer, providing clear details and citing evidence where applicable.\n",
    "# Refined answer:\n",
    "# \"\"\"\n",
    "# refine_prompt_template = \"\"\"\n",
    "# The initial answer is: {existing_answer}\n",
    "# Additional context: {context}\n",
    "\n",
    "# IMPORTANT: Before refining, ensure the question is clearly related to CSU policies.\n",
    "# If you determine that the query is off-topic, immediately respond with:\n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please ask a question related to CSU policies?\"\n",
    "# Otherwise, please refine and elaborate on the answer, providing clear details and citing evidence as needed.\n",
    "# Refined answer:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# refine_prompt_template = \"\"\"\n",
    "# [INTERNAL: If the question contains \"summarize\", \"clarify\", or \"rephrase\", do NOT output the off-topic message; simply refine the previous answer.]\n",
    "\n",
    "# The initial answer is: {existing_answer}\n",
    "# Additional context: {context}\n",
    "\n",
    "# IMPORTANT: If the new context does not clearly indicate that the question pertains to CSU policies and no follow-up instruction is present, respond with exactly:\n",
    "# \"I'm sorry, I can only answer questions related to CSU policies. Could you please ask a question related to CSU policies?\"\n",
    "# Otherwise, refine and expand the answer.\n",
    "\n",
    "# Refined answer:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "You are a CSU Policy Assistant. Your response must be strictly based on CSU policies.\n",
    "\n",
    "Step 1: Check if the question or additional context is about CSU policies.\n",
    "- If NOT, respond ONLY with:\n",
    "\"I'm sorry, I can only answer questions related to CSU policies. Could you please ask a question related to CSU policies?\"\n",
    "\n",
    "Step 2: If it IS about CSU policies, refine and expand the answer using the context provided.\n",
    "\n",
    "The initial answer is: {existing_answer}\n",
    "Additional context: {context}\n",
    "Refined answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9c92581a-2845-430d-8921-8c15f98f947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating PromptTemplate objects\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template,\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template,\n",
    "    input_variables=[\"existing_answer\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1cc72-b771-4c7f-b8a9-222f469d9817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5bd672e1-76c6-4e3b-ab33-55ee0a5c8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass RetrievalQA to allow extra chain_type_kwargs.\n",
    "class CustomRetrievalQA(RetrievalQA):\n",
    "    class Config:\n",
    "        extra = Extra.allow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "05bc7b2e-1f6f-4cae-9bd6-4a9fca88a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_csu_policy_question(query: str) -> bool:\n",
    "    \"\"\"Check if the query relates to CSU policies using keywords.\"\"\"\n",
    "    csu_keywords = [\"CSU\", \"California State University\", \"policy\", \"academic integrity\", \"code of conduct\"]\n",
    "    return any(keyword.lower() in query.lower() for keyword in csu_keywords)\n",
    "\n",
    "# Modify your retriever to return empty results for non-policy questions\n",
    "class PolicyFilteredRetriever(EnsembleRetriever):\n",
    "    def get_relevant_documents(self, query: str):\n",
    "        if not is_csu_policy_question(query):\n",
    "            return []  # Return empty list for non-policy questions\n",
    "        return super().get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124d4e8-a1e8-43b5-bd7f-f6c9cfa703d1",
   "metadata": {},
   "source": [
    "#### Creating Hybrid Retriever (Combines Semantic, Keyword & Graph Retrieval)\n",
    "\n",
    "I'm creating a is a custom hybrid retriever class that:\n",
    "1. Retrieves documents via semantic and keyword search.\n",
    "2. Uses the graph to add neighboring nodes of the retrieved chunks (for additional context).\n",
    "3. Deduplicates and returns a final list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6cff1a63-821b-4622-b417-2a9f1291f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGraphRetriever(BaseRetriever, BaseModel):\n",
    "    semantic_retriever: Any\n",
    "    keyword_retriever: Any\n",
    "    policy_graph: nx.Graph\n",
    "    top_k: int = Field(default=5)\n",
    "    graph_hops: int = Field(default=1)\n",
    "\n",
    "    class Config:\n",
    "        extra = \"allow\"\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        sem_docs = self.semantic_retriever.get_relevant_documents(query)\n",
    "        key_docs = self.keyword_retriever.get_relevant_documents(query)\n",
    "        combined = sem_docs + key_docs\n",
    "        expanded_docs = combined.copy()\n",
    "        for doc in combined:\n",
    "            for node, data in self.policy_graph.nodes(data=True):\n",
    "                if data[\"doc\"].page_content.strip() == doc.page_content.strip():\n",
    "                    neighbors = nx.single_source_shortest_path_length(self.policy_graph, node, cutoff=self.graph_hops)\n",
    "                    for n in neighbors:\n",
    "                        neighbor_doc = self.policy_graph.nodes[n][\"doc\"]\n",
    "                        expanded_docs.append(neighbor_doc)\n",
    "                    break\n",
    "        seen = {}\n",
    "        for doc in expanded_docs:\n",
    "            key = (doc.metadata.get(\"policy_title\", \"\"), doc.metadata.get(\"page\", \"\"), doc.page_content)\n",
    "            seen[key] = doc\n",
    "        unique_docs = list(seen.values())\n",
    "        return unique_docs[:self.top_k]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval is not implemented for HybridGraphRetriever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "beb223c2-8630-49cd-bb6e-169a78f8fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: HybridGraphRetriever with approximate graph is ready.\n"
     ]
    }
   ],
   "source": [
    "hybrid_retriever = HybridGraphRetriever(\n",
    "    semantic_retriever=semantic_retriever,\n",
    "    keyword_retriever=keyword_retriever,\n",
    "    policy_graph=policy_graph,\n",
    "    top_k=5,\n",
    "    graph_hops=1\n",
    ")\n",
    "\n",
    "# hybrid_retriever = PolicyFilteredRetriever(retrievers=[bm25_retriever, tfidf_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "print(\"[INFO]: HybridGraphRetriever with approximate graph is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "11d307d6-bcc3-47f4-894a-c22d396f05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom ConversationalRetrievalChain subclass that allows extra keys.\n",
    "class CustomConversationalRetrievalChain(ConversationalRetrievalChain):\n",
    "    class Config:\n",
    "        extra = Extra.allow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d9d7eae9-a69d-4e25-a6ba-bbd0ebf3df4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Custom refine documents chain is ready.\n"
     ]
    }
   ],
   "source": [
    "# 2. Create the LLMs\n",
    "llm = Ollama(model=\"mistral\", temperature=0.3)\n",
    "\n",
    "initial_llm_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
    "refine_llm_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "\n",
    "combine_docs_chain = RefineDocumentsChain(\n",
    "    initial_llm_chain=initial_llm_chain,\n",
    "    refine_llm_chain=refine_llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    initial_response_name=\"existing_answer\"\n",
    ")\n",
    "print(\"[INFO]: Custom refine documents chain is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8106dcc9-1510-4ae3-9821-bcb0903883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_question_prompt = PromptTemplate(\n",
    "    template=\"{question}\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "question_generator = LLMChain(llm=llm, prompt=dummy_question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "04807f2c-386b-4ca0-a14f-f2341f33e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Custom ConversationalRetrievalChain is ready.\n"
     ]
    }
   ],
   "source": [
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\", \n",
    "#     output_key=\"answer\", \n",
    "#     return_messages=True\n",
    "# )\n",
    "# rag_chain = CustomRetrievalQA.from_chain_type(\n",
    "#     llm=llm_for_chain,\n",
    "#     chain_type=\"refine\",\n",
    "\n",
    "\n",
    " #     retriever=hybrid_retriever,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs={\n",
    "#         \"question_prompt\": question_prompt,\n",
    "#         \"refine_prompt\": refine_prompt,\n",
    "#         \"document_variable_name\": \"context\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# rag_chain_custom = CustomConversationalRetrievalChain.from_llm(\n",
    "#     llm=llm_for_chain, \n",
    "#     retriever=hybrid_retriever,\n",
    "#     memory=memory,\n",
    "#     output_key=\"answer\",\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs={\n",
    "#          \"question_prompt\": question_prompt,\n",
    "#          \"refine_prompt\": refine_prompt,\n",
    "#          \"document_variable_name\": \"context\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain(\n",
    "    retriever=hybrid_retriever,               \n",
    "    combine_docs_chain=combine_docs_chain,   \n",
    "    question_generator=question_generator,\n",
    "    memory=memory,\n",
    "    output_key=\"answer\",\n",
    "    return_source_documents=True,\n",
    "    callbacks=[]\n",
    ")\n",
    "print(\"[✅] Custom ConversationalRetrievalChain is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "93c9246a-1f8e-43ec-bda2-188adf2ed652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_on_topic(question: str, llm) -> bool:\n",
    "    \"\"\"Use a simple prompt to ask the LLM whether the query is related to CSU policies.\"\"\"\n",
    "    check_prompt = PromptTemplate(\n",
    "        template=\"Is the following question related to CSU policies? Answer with 'yes' or 'no'.\\nQuestion: {question}\",\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    check_chain = LLMChain(llm=llm, prompt=check_prompt)\n",
    "    response = check_chain.predict(question=question)\n",
    "    return \"yes\" in response.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136bbb8-154e-4fd2-a68a-4b0d6671975a",
   "metadata": {},
   "source": [
    "### Creating  a Simple Intent Classifier Using the Existing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b48df5b5-3c93-4ecb-9146-8308ee2294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleIntentClassifier:\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"Initialize a simple intent classifier using an existing LLM\"\"\"\n",
    "        self.llm = llm\n",
    "        self.intents = [\n",
    "            \"policy_lookup\",\n",
    "            \"policy_comparison\",\n",
    "            \"policy_application\",\n",
    "            \"summarize_previous\",\n",
    "            \"clarification\",\n",
    "            \"out_of_scope\"\n",
    "        ]\n",
    "        \n",
    "        # Create the classification prompt\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"\n",
    "            Classify the following query into exactly one of these intents:\n",
    "            - policy_lookup: Questions about what a specific CSU policy is or contains\n",
    "            - policy_comparison: Questions comparing two or more CSU policies\n",
    "            - policy_application: Questions about how a CSU policy applies to a situation\n",
    "            - summarize_previous: Requests to summarize previous information\n",
    "            - clarification: Requests to clarify previous information\n",
    "            - out_of_scope: Questions not related to CSU policies\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Also extract any policy entities mentioned in the query.\n",
    "            \n",
    "            Respond in this exact format:\n",
    "            Intent: [intent name]\n",
    "            Confidence: [0.0-1.0]\n",
    "            Entities: [list of policy entities or \"none\"]\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
    "    \n",
    "    def parse(self, query):\n",
    "        \"\"\"Parse the query and return intent classification\"\"\"\n",
    "        result = self.chain.run(query=query)\n",
    "        \n",
    "        # Extract intent, confidence, and entities using regex\n",
    "        intent_match = re.search(r\"Intent: (\\w+)\", result)\n",
    "        confidence_match = re.search(r\"Confidence: (0\\.\\d+|1\\.0)\", result)\n",
    "        entities_match = re.search(r\"Entities: (.+)\", result)\n",
    "        \n",
    "        intent = intent_match.group(1) if intent_match else \"out_of_scope\"\n",
    "        confidence = float(confidence_match.group(1)) if confidence_match else 0.5\n",
    "        \n",
    "        entities = []\n",
    "        if entities_match and \"none\" not in entities_match.group(1).lower():\n",
    "            entity_names = entities_match.group(1).strip(\"[]\").split(\",\")\n",
    "            for entity in entity_names:\n",
    "                entity = entity.strip()\n",
    "                if entity:\n",
    "                    entities.append({\"entity\": \"policy\", \"value\": entity})\n",
    "        \n",
    "        return {\n",
    "            \"intent\": {\"name\": intent, \"confidence\": confidence},\n",
    "            \"entities\": entities\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6082b0-8d1f-47d0-a1c2-3c67355cc64e",
   "metadata": {},
   "source": [
    "### Implementing Conversation Context Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "feb0ebee-f4c7-463c-8567-f3b5b46b3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyConversationContext:\n",
    "    def __init__(self, max_turns=5):\n",
    "        self.context = []\n",
    "        self.max_turns = max_turns\n",
    "        self.current_policies = set()\n",
    "        self.last_answer = \"\"\n",
    "        self.last_intent = None\n",
    "        self.last_source_docs = []\n",
    "\n",
    "    def add_turn(self, user_message, bot_response, intent, entities=None, source_docs=None):\n",
    "        # Adding new conversation turn\n",
    "        self.context.append({\n",
    "            \"user\": user_message,\n",
    "            \"bot\": bot_response,\n",
    "            \"intent\": intent,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "\n",
    "        # Updating tracking variables\n",
    "        self.last_answer = bot_response\n",
    "        self.last_intent = intent\n",
    "        if source_docs:\n",
    "            self.last_source_docs = source_docs\n",
    "\n",
    "        # Tracking mentioned policies\n",
    "        if entities:\n",
    "            for entity in entities:\n",
    "                if entity[\"entity\"] == \"policy\":\n",
    "                    self.current_policies.add(entity[\"value\"])\n",
    "        \n",
    "        # Maintaining context window size\n",
    "        if len(self.context) > self.max_turns:\n",
    "            self.context.pop(0)\n",
    "    \n",
    "        def get_last_answer(self):\n",
    "            return self.last_answer\n",
    "        \n",
    "        def get_last_source_docs(self):\n",
    "            return self.last_source_docs\n",
    "        \n",
    "        def get_relevant_policies(self):\n",
    "            return list(self.current_policies)\n",
    "        \n",
    "        def get_context_history(self):\n",
    "            return self.context\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c289d6-1dd5-456b-b7cd-d1a32dd8e35e",
   "metadata": {},
   "source": [
    "### Creating an Intent-Aware Retriever Wrapper\n",
    "Wrapping our existing hybrid retriever with intent-aware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6fee99e2-4df5-4bb9-b100-586bacd84e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentAwareRetrieverWrapper:\n",
    "    def __init__(self, hybrid_retriever, intent_classifier):\n",
    "        self.hybrid_retriever = hybrid_retriever\n",
    "        self.intent_classifier = intent_classifier\n",
    "    \n",
    "    def get_relevant_documents(self, query):\n",
    "        # Classify intent\n",
    "        intent_result = self.intent_classifier.parse(query)\n",
    "        intent = intent_result[\"intent\"][\"name\"]\n",
    "        confidence = intent_result[\"intent\"][\"confidence\"]\n",
    "        entities = intent_result.get(\"entities\", [])\n",
    "        \n",
    "        # Handle special intents\n",
    "        if intent in [\"summarize_previous\", \"clarification\"]:\n",
    "            return []\n",
    "            \n",
    "        if intent == \"out_of_scope\" and confidence > 0.6:\n",
    "            return []\n",
    "        \n",
    "        # For policy comparison, enhance retrieval\n",
    "        if intent == \"policy_comparison\":\n",
    "            policy_entities = [e[\"value\"] for e in entities if e[\"entity\"] == \"policy\"]\n",
    "            if len(policy_entities) >= 2:\n",
    "                all_docs = []\n",
    "                for policy in policy_entities:\n",
    "                    enhanced_query = f\"{policy} policy CSU\"\n",
    "                    docs = self.hybrid_retriever.get_relevant_documents(enhanced_query)\n",
    "                    all_docs.extend(docs)\n",
    "                return all_docs\n",
    "        \n",
    "        # For regular policy questions, use the hybrid retriever\n",
    "        return self.hybrid_retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "67265f42-0249-44b2-8b53-b15d8c2c37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Decomposition Using spaCy\n",
    "def simple_decompose_query(query):\n",
    "    \"\"\"Simple query decomposition based on common patterns\"\"\"\n",
    "    # Check for comparison queries\n",
    "    comparison_keywords = [\"compare\", \"difference\", \"versus\", \"vs\", \"similarities\", \"differences\"]\n",
    "    is_comparison = any(keyword in query.lower() for keyword in comparison_keywords)\n",
    "    \n",
    "    if is_comparison:\n",
    "        # This is a comparison query, handle it as is\n",
    "        return [query]\n",
    "    \n",
    "    # Split on question marks for multiple questions\n",
    "    if \"?\" in query:\n",
    "        parts = query.split(\"?\")\n",
    "        # Filter out empty parts and add back the question marks\n",
    "        return [part.strip() + \"?\" for part in parts if part.strip()]\n",
    "    \n",
    "    # Not a complex query\n",
    "    return [query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "af495fa0-7115-49e2-a0d5-9afe783d424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policy_chatbot(query: str):\n",
    "\n",
    "#     # First, check if the query is on-topic.\n",
    "#     if not is_on_topic(query, llm):\n",
    "#         print(\"\\nI'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\\n\")\n",
    "#         return\n",
    "    \n",
    "#     # Note: Use \"question\" as the input key.\n",
    "#     result = rag_chain({\"question\": query})\n",
    "#     answer = result.get(\"answer\", \"\")\n",
    "#     source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "#     print(f\"\\n💬 Query: {query}\\n\")\n",
    "#     print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "    \n",
    "#     print(\"📚 References:\")\n",
    "#     for i, doc in enumerate(source_docs, start=1):\n",
    "#         metadata = doc.metadata\n",
    "#         policy_title = metadata.get(\"policy_title\", \"Unknown Policy\")\n",
    "#         policy_url = metadata.get(\"policy_url\", None)\n",
    "#         page = metadata.get(\"page\")\n",
    "#         page_num = page + 1 if isinstance(page, int) else \"?\"\n",
    "#         if policy_url and isinstance(policy_url, str) and policy_url.startswith(\"http\"):\n",
    "#             link = f\"{policy_url}#page={page_num} ({policy_title})\"\n",
    "#         else:\n",
    "#             link = f\"{policy_title} (Page {page_num})\"\n",
    "#         print(f\"[{i}] {link}\")\n",
    "#     print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "# def policy_chatbot(question: str):  \n",
    "#     result = rag_chain({\"question\": question})\n",
    "#     answer = result.get(\"answer\", \"\")\n",
    "#     source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "#     print(f\"\\n💬 Query: {question}\\n\")\n",
    "#     print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "#     print(\"📚 References:\")\n",
    "#     for i, doc in enumerate(source_docs, 1):\n",
    "#         meta = doc.metadata\n",
    "#         title = meta.get(\"policy_title\", \"Unknown Policy\")\n",
    "#         url = meta.get(\"policy_url\", \"\")\n",
    "#         page = meta.get(\"page\")\n",
    "#         page_disp = page + 1 if isinstance(page, int) else \"?\"\n",
    "#         if url:\n",
    "#             print(f\"[{i}] {url}#page={page_disp} ({title})\")\n",
    "#         else:\n",
    "#             print(f\"[{i}] {title} (Page {page_disp})\")\n",
    "#     print(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "787fee27-268f-43ae-b2d3-5be764e924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_on_topic(question: str) -> bool:\n",
    "#     \"\"\"\n",
    "#     Uses a simple LLMChain to check if the question is directly related to CSU policies.\n",
    "#     Returns True if the answer is 'yes', otherwise False.\n",
    "#     \"\"\"\n",
    "#     on_topic_prompt = PromptTemplate(\n",
    "#         template=\"Is the following question related to CSU policies? Answer only 'yes' or 'no'.\\nQuestion: {question}\",\n",
    "#         input_variables=[\"question\"]\n",
    "#     )\n",
    "#     on_topic_chain = LLMChain(llm=llm, prompt=on_topic_prompt)\n",
    "#     response = on_topic_chain.predict(question=question)\n",
    "#     return \"yes\" in response.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1322b753-2039-4776-bcfa-f8e801b580d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policy_chatbot(question: str):\n",
    "#     # Pre-check: if question is off-topic, immediately return the fixed off-topic message.\n",
    "#     if not is_on_topic(question):\n",
    "#         print(\"\\nI'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\\n\")\n",
    "#         return\n",
    "\n",
    "#     result = rag_chain({\"question\": question})\n",
    "#     answer = result.get(\"answer\", \"\")\n",
    "#     source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "#     print(f\"\\n💬 Query: {question}\\n\")\n",
    "#     print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "#     print(\"📚 References:\")\n",
    "#     for i, doc in enumerate(source_docs, start=1):\n",
    "#         meta = doc.metadata\n",
    "#         title = meta.get(\"policy_title\", \"Unknown Policy\")\n",
    "#         url = meta.get(\"policy_url\", \"\")\n",
    "#         page = meta.get(\"page\")\n",
    "#         page_disp = page + 1 if isinstance(page, int) else \"?\"\n",
    "#         if url and isinstance(url, str) and url.startswith(\"http\"):\n",
    "#             print(f\"[{i}] {url}#page={page_disp} ({title})\")\n",
    "#         else:\n",
    "#             print(f\"[{i}] {title} (Page {page_disp})\")\n",
    "#     print(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9d263fd8-b11b-4851-b013-1094f5bf43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_chatbot(question: str):\n",
    "    \"\"\"Enhanced policy chatbot with NLU capabilities\"\"\"\n",
    "    global intent_classifier, context_manager\n",
    "    \n",
    "    # Ensure components are initialized\n",
    "    if 'intent_classifier' not in globals() or 'context_manager' not in globals():\n",
    "        initialize_chatbot()\n",
    "    \n",
    "    # Classify intent\n",
    "    intent_result = intent_classifier.parse(question)\n",
    "    intent = intent_result[\"intent\"][\"name\"]\n",
    "    confidence = intent_result[\"intent\"][\"confidence\"]\n",
    "    entities = intent_result.get(\"entities\", [])\n",
    "    \n",
    "    print(f\"\\n💬 Query: {question}\\n\")\n",
    "    \n",
    "    # Handle special intents\n",
    "    if intent == \"summarize_previous\":\n",
    "        last_answer = context_manager.get_last_answer()\n",
    "        if not last_answer:\n",
    "            answer = \"I don't have any previous information to summarize.\"\n",
    "        else:\n",
    "            # Use a simple summarization prompt\n",
    "            summarization_prompt = f\"Summarize the following text in a concise way:\\n\\n{last_answer}\"\n",
    "            answer = llm_for_chain.predict(text=summarization_prompt)\n",
    "        \n",
    "        print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "        context_manager.add_turn(question, answer, intent)\n",
    "        return\n",
    "    \n",
    "    if intent == \"clarification\":\n",
    "        last_answer = context_manager.get_last_answer()\n",
    "        if not last_answer:\n",
    "            answer = \"I'm sorry, but I don't have any previous information to clarify. Could you ask a specific question about CSU policies?\"\n",
    "        else:\n",
    "            clarification_prompt = f\"The user is asking for clarification on this response: '{last_answer}'. Provide a clearer explanation.\"\n",
    "            answer = llm_for_chain.predict(text=clarification_prompt)\n",
    "        \n",
    "        print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "        context_manager.add_turn(question, answer, intent)\n",
    "        return\n",
    "    \n",
    "    # Check if it's a non-policy question\n",
    "    if intent == \"out_of_scope\" and confidence > 0.6:\n",
    "        answer = \"I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\"\n",
    "        print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "        context_manager.add_turn(question, answer, intent)\n",
    "        return\n",
    "    \n",
    "    # Use your existing RAG chain with the original hybrid_retriever\n",
    "    # We'll handle the intent-aware filtering here\n",
    "    \n",
    "    # Check if query needs decomposition\n",
    "    sub_queries = simple_decompose_query(question)\n",
    "    \n",
    "    if len(sub_queries) > 1:\n",
    "        # Process each sub-query and combine results\n",
    "        combined_answer = \"\"\n",
    "        all_sources = []\n",
    "        \n",
    "        for sub_q in sub_queries:\n",
    "            # Use your existing rag_chain\n",
    "            result = rag_chain({\"question\": sub_q})\n",
    "            sub_answer = result.get(\"answer\", \"\")\n",
    "            if sub_answer:\n",
    "                combined_answer += sub_answer + \"\\n\\n\"\n",
    "                all_sources.extend(result.get(\"source_documents\", []))\n",
    "        \n",
    "        answer = combined_answer.strip()\n",
    "        source_docs = all_sources\n",
    "    else:\n",
    "        # Process normally for simple queries\n",
    "        result = rag_chain({\"question\": question})\n",
    "        answer = result.get(\"answer\", \"\")\n",
    "        source_docs = result.get(\"source_documents\", [])\n",
    "    \n",
    "    # Format the response based on intent\n",
    "    if intent == \"policy_comparison\":\n",
    "        policy_entities = [e[\"value\"] for e in entities if e[\"entity\"] == \"policy\"]\n",
    "        if len(policy_entities) >= 2:\n",
    "            answer = format_comparison_response(answer, policy_entities)\n",
    "    \n",
    "    # Update conversation context\n",
    "    context_manager.add_turn(question, answer, intent, entities, source_docs)\n",
    "    \n",
    "    # Output the answer\n",
    "    print(f\"🤖 Answer:\\n{answer}\\n\")\n",
    "    \n",
    "    # Print references\n",
    "    print(\"📚 References:\")\n",
    "    for i, doc in enumerate(source_docs, start=1):\n",
    "        meta = doc.metadata\n",
    "        title = meta.get(\"policy_title\", \"Unknown Policy\")\n",
    "        url = meta.get(\"policy_url\", \"\")\n",
    "        page = meta.get(\"page\")\n",
    "        page_disp = page + 1 if isinstance(page, int) else \"?\"\n",
    "        if url and isinstance(url, str) and url.startswith(\"http\"):\n",
    "            print(f\"[{i}] {url}#page={page_disp} ({title})\")\n",
    "        else:\n",
    "            print(f\"[{i}] {title} (Page {page_disp})\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b1983bd5-1c0a-493c-9198-4d5f69ca0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comparison_response(answer, policy_entities):\n",
    "    \"\"\"Format the response as a comparison table for policy comparison intents\"\"\"\n",
    "    # Create a header for the comparison\n",
    "    comparison = f\"## Comparison of {' and '.join(policy_entities)}\\n\\n\"\n",
    "    \n",
    "    # Try to extract key aspects for comparison\n",
    "    aspects = [\"Focus\", \"Scope\", \"Enforcement\", \"Penalties\", \"Application\"]\n",
    "    \n",
    "    # Create a markdown table\n",
    "    comparison += \"| Aspect | \" + \" | \".join(policy_entities) + \" |\\n\"\n",
    "    comparison += \"|--------|\" + \"|\".join([\"---------\" for _ in policy_entities]) + \"|\\n\"\n",
    "    \n",
    "    # Add the original answer after the table\n",
    "    comparison += \"\\n\\n\" + answer\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def format_policy_lookup_response(answer, policy_name):\n",
    "    \"\"\"Format the response for policy lookup intents\"\"\"\n",
    "    formatted = f\"## {policy_name.title()} Policy\\n\\n\"\n",
    "    \n",
    "    # Add key points section\n",
    "    formatted += \"### Key Points:\\n\"\n",
    "    \n",
    "    # Add the original answer\n",
    "    formatted += \"\\n\" + answer\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "681be90b-66e1-4349-b7f3-8e187408aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chatbot():\n",
    "    \"\"\"Initialize all components for the enhanced policy chatbot\"\"\"\n",
    "    global intent_classifier, context_manager\n",
    "    \n",
    "    # Initialize NLU components if not already initialized\n",
    "    if 'intent_classifier' not in globals():\n",
    "        print(\"Initializing simple intent classifier...\")\n",
    "        intent_classifier = SimpleIntentClassifier(llm_for_chain)\n",
    "    \n",
    "    if 'context_manager' not in globals():\n",
    "        print(\"Initializing conversation context manager...\")\n",
    "        context_manager = PolicyConversationContext()\n",
    "    \n",
    "    # Create the intent-aware retriever wrapper\n",
    "    intent_aware_retriever = IntentAwareRetrieverWrapper(hybrid_retriever, intent_classifier)\n",
    "    \n",
    "    print(\"Enhanced policy chatbot initialized successfully!\")\n",
    "    return intent_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d951f0b4-0d99-40c1-b136-ddc2f1605e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced policy chatbot initialized successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.IntentAwareRetrieverWrapper at 0x7fd1d7e99040>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize all components\n",
    "initialize_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ce6b72bd-d335-42fb-a939-3e9fe4d008ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: What is the academic integrity policy?\n",
      "\n",
      "🤖 Answer:\n",
      " The California State University (CSU) operates under a budget system, where the authorizations contained in the previous budget are used until the new one is approved. Each campus has a Budget Review Board, which is convened annually prior to budget preparation. This board includes the Associated Students' President and the Dean of Students, among others.\n",
      "\n",
      "The role of this review board is to familiarize themselves with the procedures involved in the budget process. While the CSU values academic integrity and encourages responsible behaviors from students, there may be instances where a student's behavior does not align with the Student Conduct Code. In such cases, an educational process is initiated to promote safety and good citizenship, and appropriate consequences may be imposed according to the CSU policy. These consequences can range from receiving a lower grade on an assignment or in a course, being placed on academic probation, suspension, or even expulsion from the university.\n",
      "\n",
      "Regarding your additional context about ensuring vulnerabilities are addressed in a production environment, this falls under the purview of the CSU Information Security Policy and Standards. The policy addresses various security concerns such as un-validated input, injection flaws, inadequate access control, and improper error handling. For more specific details about this policy, please consult the CSU Information Security Policy and Standards available at http://calstate.policystat.com/policy/15698973/. If you have any further questions or need clarification on any aspect of CSU policies, feel free to ask!\n",
      "\n",
      "📚 References:\n",
      "[1] https://calstate.policystat.com/policy/8843080/#page=5 (resolutions of the board of trustees 2008)\n",
      "[2] https://calstate.policystat.com/policy/15274496/#page=63 (csu mandatory catalog copy)\n",
      "[3] https://calstate.policystat.com/policy/15274496/#page=63 (csu mandatory catalog copy)\n",
      "[4] Procedures for Preparation- Review and Approval of Associated Students- Budgets (Page 4)\n",
      "[5] https://calstate.policystat.com/policy/15698973/#page=50 (csu information security policy and standards)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_chatbot(\"What is the academic integrity policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70664aa3-8bc8-4b4a-aae3-2596b9b8f5f1",
   "metadata": {},
   "source": [
    "#### Trying our chatbot with different queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8aa702ca-9a1d-4d36-b14b-280ca99230a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: What are the approval procedures for academic freedom-related policies?\n",
      "\n",
      "🤖 Answer:\n",
      " The process for conferring the title of President Emeritus at California State University (CSU) involves a review of an individual's valuable contributions to their university and to this system of higher education, as outlined in Resolution RBOT 07-03-07 for conferring the Title Trustee Emeritus and Agenda Item 2 of the Committee on Educational Policy at the March 16-17, 2010 Board Meeting.\n",
      "\n",
      "In cases where there is disagreement with this determination, it should be noted on the outside employment disclosure form and escalated to the next level of review. This second and final level of review should be conducted by an independent review committee appointed by the President or Chancellor or his/her designee. The recommendation provided at this level shall be the final determination.\n",
      "\n",
      "Regarding your additional context, it is important to note that the process does not involve using questionnaires or other survey instruments in connection with the review process. Additionally, petitions and anonymous or unsigned feedback will not be considered in this process. Confidentiality is preserved as much as possible throughout the review process, including written reports.\n",
      "\n",
      "For senior management employees, the document review and approval process may vary depending on specific policies and procedures within each campus. It is recommended to consult the respective campus policies for more detailed information.\n",
      "\n",
      "📚 References:\n",
      "[1] https://calstate.policystat.com/policy/15062388/#page=2 (doctor of education degree proposal process)\n",
      "[2] https://calstate.policystat.com/policy/16054531/#page=2 (board of trustees policy and procedures for review of presidents)\n",
      "[3] https://calstate.policystat.com/policy/8843098/#page=6 (resolutions of the board of trustees 2010)\n",
      "[4] https://calstate.policystat.com/policy/9102711/#page=34 (resolutions of the board of trustees 2003)\n",
      "[5] Revised Outside Employment Disclosure Requirements for Management Personnel Plan -MPP- and Executive Employees (Page 5)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_chatbot(\"What are the approval procedures for academic freedom-related policies?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ccc8fd83-713f-43ef-a706-8bf7695783cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: How many states are there in USA?\n",
      "\n",
      "🤖 Answer:\n",
      "I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How many states are there in USA?\"\n",
    "policy_chatbot(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "15767702-94af-49d3-b89c-75d3dba42156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: What is the capital of France?\n",
      "\n",
      "🤖 Answer:\n",
      "I'm sorry, I can only answer questions related to CSU policies. Could you please rephrase your query accordingly?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_chatbot(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61d1c5d3-3dfa-43a4-9e71-1504ad2bd4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: What is the annual fees of MS in CS fees at San Jose State University?\n",
      "\n",
      "🤖 Answer:\n",
      " According to the provided context, the consultation regarding future assessments of approved tuition schedules within the California State University (CSU) system will begin in August, involving CSSA leadership-elect, students, faculty, and staff. The assessment prepared by the Chancellor's Office will include a comparison of CSU systemwide tuition rates to public four-year institutions of higher education in the United States as reported.\n",
      "\n",
      "Regarding the Physical Science Replacement Building, Wing A at California State University, Los Angeles (CSULA), it has been prepared in accordance with the requirements of the California Environmental Quality Act (CEQA). The proposed project is expected to have no significant adverse impacts on the environment and will benefit the CSU. The schematic plans for this project have been approved at a cost of $42,595,000 at CCCI 4019.\n",
      "\n",
      "For the most accurate and up-to-date information about specific campuses and their associated fees or projects, I recommend visiting the official website of the CSU campus where you are interested in pursuing your studies or contacting their admissions office directly. The California State University (CSU) policy on fees is subject to change, so it's always best to verify the current fee structure from an authoritative source. Additionally, for more detailed information about specific projects and their environmental impact assessments, you may want to consult the CEQA documentation available on the respective campus or system websites.\n",
      "\n",
      "📚 References:\n",
      "[1] Category III Fees- Category IV Fee- San Francisco State University (Page 1)\n",
      "[2] Student Success- Excellence and Technology Fee- San José State University (Page 1)\n",
      "[3] https://calstate.policystat.com/policy/16681992/#page=7 (student tuition and fee policy)\n",
      "[4] https://calstate.policystat.com/policy/16681992/#page=3 (student tuition and fee policy)\n",
      "[5] https://calstate.policystat.com/policy/9102711/#page=43 (resolutions of the board of trustees 2003)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the annual fees of MS in CS fees at San Jose State University?\"\n",
    "policy_chatbot(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59928b-c77d-4177-8eeb-76e0a647aeff",
   "metadata": {},
   "source": [
    "#### Experimenting complex queries\n",
    "\n",
    "This below query requires the chatbot to retrieve information from the Academic Freedom Policy (which discusses research freedom and potential conflicts) and additional policy documents related to research funding or conflict of interest. The answer must integrate details from more than one policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ad5cba20-0902-4cd5-a6c4-cbe923f8c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Query: How does the university policy address conflicts between faculty research priorities and commercial interests, and what approval procedures are in place to manage these conflicts?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpolicy_chatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow does the university policy address conflicts between faculty research priorities and commercial interests, and what approval procedures are in place to manage these conflicts?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[179], line 73\u001b[0m, in \u001b[0;36mpolicy_chatbot\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     70\u001b[0m     source_docs \u001b[38;5;241m=\u001b[39m all_sources\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Process normally for simple queries\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     answer \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     source_docs \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[0m, in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwarning_emitting_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;124;03m        *args: The positional arguments to the function.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m        **kwargs: The keyword arguments to the function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m warned\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warned \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_caller_internal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    384\u001b[0m }\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    387\u001b[0m     inputs,\n\u001b[1;32m    388\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m--> 389\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    390\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    391\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    171\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m try:\n\u001b[1;32m    155\u001b[0m     self._validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs = (\n\u001b[1;32m    157\u001b[0m         self._call(inputs, run_manager=run_manager)\n\u001b[1;32m    158\u001b[0m         if new_arg_supported\n\u001b[1;32m    159\u001b[0m         else self._call(inputs)\n\u001b[0;32m--> 160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: dict[str, Any] = self.prep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m except BaseException as e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/conversational_retrieval/base.py:170\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    168\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[1;32m    169\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[0;32m--> 170\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_inputs\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[0m, in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwarning_emitting_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;124;03m        *args: The positional arguments to the function.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m        **kwargs: The keyword arguments to the function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m warned\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warned \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_caller_internal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:611\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[0m, in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwarning_emitting_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;124;03m        *args: The positional arguments to the function.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m        **kwargs: The keyword arguments to the function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m warned\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warned \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_caller_internal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    384\u001b[0m }\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    387\u001b[0m     inputs,\n\u001b[1;32m    388\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m--> 389\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    390\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    391\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    171\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m try:\n\u001b[1;32m    155\u001b[0m     self._validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs = (\n\u001b[1;32m    157\u001b[0m         self._call(inputs, run_manager=run_manager)\n\u001b[1;32m    158\u001b[0m         if new_arg_supported\n\u001b[1;32m    159\u001b[0m         else self._call(inputs)\n\u001b[0;32m--> 160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: dict[str, Any] = self.prep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m except BaseException as e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/combine_documents/base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/combine_documents/refine.py:170\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     base_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_refine_inputs(doc, res)\n\u001b[1;32m    169\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 170\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine_llm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     refine_steps\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(refine_steps, res)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/llm.py:318\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m        callbacks: Callbacks to pass to LLMChain\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m        **kwargs: Keys to pass to prompt template.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m        Completion from LLM.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        .. code-block:: python\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[0m, in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwarning_emitting_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;124;03m        *args: The positional arguments to the function.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m        **kwargs: The keyword arguments to the function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m        The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m warned\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warned \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_caller_internal():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    384\u001b[0m }\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m    387\u001b[0m     inputs,\n\u001b[1;32m    388\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m--> 389\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[1;32m    390\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[1;32m    391\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    171\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m try:\n\u001b[1;32m    155\u001b[0m     self._validate_inputs(inputs)\n\u001b[1;32m    156\u001b[0m     outputs = (\n\u001b[1;32m    157\u001b[0m         self._call(inputs, run_manager=run_manager)\n\u001b[1;32m    158\u001b[0m         if new_arg_supported\n\u001b[1;32m    159\u001b[0m         else self._call(inputs)\n\u001b[0;32m--> 160\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     final_outputs: dict[str, Any] = self.prep_outputs(\n\u001b[1;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m except BaseException as e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    124\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    140\u001b[0m         prompts,\n\u001b[1;32m    141\u001b[0m         stop,\n\u001b[1;32m    142\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/language_models/llms.py:760\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    758\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m    759\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 760\u001b[0m     callbacks: Optional[Union[Callbacks, \u001b[38;5;28mlist\u001b[39m[Callbacks]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    763\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/language_models/llms.py:963\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m )\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    960\u001b[0m             [prompt],\n\u001b[1;32m    961\u001b[0m             invocation_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    962\u001b[0m             options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m--> 963\u001b[0m             name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    964\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(prompts),\n\u001b[1;32m    965\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mrun_id_,\n\u001b[1;32m    966\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m callback_manager, prompt, run_name, run_id_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    968\u001b[0m             callback_managers, prompts, run_name_list, run_ids_list\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m     ]\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[1;32m    972\u001b[0m         prompts,\n\u001b[1;32m    973\u001b[0m         stop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/language_models/llms.py:784\u001b[0m, in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m     prompt_strings = [p.to_string() for p in prompts]\n\u001b[1;32m    775\u001b[0m     return await self.agenerate(\n\u001b[1;32m    776\u001b[0m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    779\u001b[0m def _generate_helper(\n\u001b[1;32m    780\u001b[0m     self,\n\u001b[1;32m    781\u001b[0m     prompts: list[str],\n\u001b[1;32m    782\u001b[0m     stop: Optional[list[str]],\n\u001b[1;32m    783\u001b[0m     run_managers: list[CallbackManagerForLLMRun],\n\u001b[0;32m--> 784\u001b[0m     *,\n\u001b[1;32m    785\u001b[0m     new_arg_supported: bool,\n\u001b[1;32m    786\u001b[0m     **kwargs: Any,\n\u001b[1;32m    787\u001b[0m ) -> LLMResult:\n\u001b[1;32m    788\u001b[0m     try:\n\u001b[1;32m    789\u001b[0m         output = (\n\u001b[1;32m    790\u001b[0m             self._generate(\n\u001b[1;32m    791\u001b[0m                 prompts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    798\u001b[0m             else self._generate(prompts, stop=stop)\n\u001b[1;32m    799\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_community/llms/ollama.py:437\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 437\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_community/llms/ollama.py:349\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    342\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    348\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_stream_response_to_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_community/llms/ollama.py:194\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    192\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    193\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    195\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    196\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    197\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 572\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/response.py:1138\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_chatbot(\"How does the university policy address conflicts between faculty research priorities and commercial interests, and what approval procedures are in place to manage these conflicts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11259d7d-f237-497a-9507-20ec9aa9854d",
   "metadata": {},
   "source": [
    "**Complex Query-2:**\n",
    "\n",
    "This below query demands a comparison between two distinct policies. The policystat chatbot needs to extract guidelines from both the Academic Access Policy and the Student Code of Conduct (or similar documents) and then perform a synthesis to highlight the differences and impacts on enforcement.\n",
    "\n",
    "**How it works:**\n",
    "1. The RAG pipeline retrieves chunks from both policies—semantic search picks up nuanced guidelines while keyword search fetches exact phrases like “faculty responsibilities” or “enforcement.”\n",
    "2. GraphRAG further enhances the process by connecting sections that use similar language across policies.\n",
    "3. The LLM then collates these details into a comparative answer with contextual references that indicate the policy source and page number for each piece of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600000d4-e98b-47fa-a09d-621f8307c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_chatbot(\"What are the key differences between the Academic Access Policy and the Student Code of Conduct regarding faculty responsibilities, and how do these differences influence policy enforcement at the institution?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4fa193-7a5a-40aa-ae64-206ef3e36fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33015e97-1f15-4ed9-8880-0dff109a74a3",
   "metadata": {},
   "source": [
    "**Complex Query-3:**\n",
    "The below query is a multi-faceted query requiring integration of information from several policies. It involves not only the Academic Freedom Policy but also the tenure guidelines and research compliance standards. The answer must present a holistic view that outlines both academic independence and regulatory compliance.\n",
    "\n",
    "**How chatbot works**:\n",
    "The chatbot uses the hybrid retrieval module to gather relevant documents from all three policy areas. Semantic retrieval captures conceptual links about “independence” and “compliance,” while keyword retrieval hones in on technical terms like “tenure” or “regulations.” The graph-based component connects these overlapping concepts across multiple documents. With conversational memory, the system preserves context across turns, and the final answer generated by the LLM includes inline citations that reference the exact policy and page number where each requirement is stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4f64d-9f4d-4f1c-b07e-0223c9f61d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Considering the university’s policies on academic freedom, tenure, and research compliance, what are the combined requirements for faculty to maintain academic independence while ensuring adherence to institutional regulations? Give me the brief summary of the entire answer in the end.\"\n",
    "policy_chatbot(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ab6f8-0030-417c-882c-0fcaa3ca9ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba7461-a42a-42b1-9e21-2f94fe65c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
